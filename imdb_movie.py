# -*- coding: utf-8 -*-
"""IMDB movie.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VjOH6ZsHoV9u5yxodwCjRIMmuMC_3km2
"""

# Step 1: Install Required Libraries
!pip install scikit-learn pandas numpy matplotlib seaborn

# Step 2: Import Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import zipfile
import os

# Step 3: Upload the File
from google.colab import files
uploaded = files.upload()

# Step 4: Load the Dataset (replace filename if different)
# Unzip the file
for fn in uploaded.keys():
  with zipfile.ZipFile(fn, 'r') as zip_ref:
    zip_ref.extractall('.')

# Assuming the extracted file is 'IMDb Movies India.csv'
df = pd.read_csv('IMDB.csv.zip', encoding='latin1')
df.head()

# Step 5: Basic Cleaning
df = df.dropna(subset=['Rating'])  # Drop rows with no ratings
df = df.drop_duplicates()

# Clean 'Duration' column to extract minutes
df['Duration'] = df['Duration'].str.extract('(\d+)').astype(float)

# Clean 'Year' column to extract year
df['Year'] = df['Name'].str.extract(r'\((\d{4})\)').astype(float)

# Convert Votes to numeric
df['Votes'] = pd.to_numeric(df['Votes'], errors='coerce')

# Fill missing values
df['Votes'].fillna(df['Votes'].median(), inplace=True)
df['Duration'].fillna(df['Duration'].median(), inplace=True)
df['Year'].fillna(df['Year'].median(), inplace=True)

# Step 6: Select Relevant Features
df_model = df[['Genre', 'Director', 'Actor 1', 'Actor 2', 'Actor 3', 'Year', 'Duration', 'Votes', 'Rating']]

# One-hot encode categorical features (keep top 20 frequent only)
def encode_top_categories(df, column, top_n=20):
    top_categories = df[column].value_counts().nlargest(top_n).index
    return pd.get_dummies(df[column].where(df[column].isin(top_categories), 'Other'), prefix=column)

# Handle potential empty genre entries
genre_encoded = df['Genre'].dropna().str.split(',', expand=True).stack().str.strip().reset_index(level=1, drop=True)
df_genre = pd.get_dummies(genre_encoded).groupby(level=0).sum()

director_encoded = encode_top_categories(df, 'Director')
actor1_encoded = encode_top_categories(df, 'Actor 1')
actor2_encoded = encode_top_categories(df, 'Actor 2')
actor3_encoded = encode_top_categories(df, 'Actor 3')

# Step 7: Combine Features
# Align indices before concatenation
y = df['Rating']
df_genre = df_genre.reindex(y.index, fill_value=0) # Fill missing indices with 0

X = pd.concat([
    df[['Year', 'Duration', 'Votes']],
    df_genre,
    director_encoded.reindex(y.index, fill_value=0),
    actor1_encoded.reindex(y.index, fill_value=0),
    actor2_encoded.reindex(y.index, fill_value=0),
    actor3_encoded.reindex(y.index, fill_value=0)
], axis=1)


# Step 8: Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 9: Train a Model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Step 10: Evaluate
y_pred = model.predict(X_test)
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))
print("R² Score:", r2_score(y_test, y_pred))

# Optional: Plot Predictions vs True Ratings
plt.figure(figsize=(8,6))
sns.scatterplot(x=y_test, y=y_pred)
plt.xlabel("Actual Rating")
plt.ylabel("Predicted Rating")
plt.title("Actual vs Predicted Movie Ratings")
plt.show()

|+/.mjnhcxezsasxervbm.l/|

\j5rhgs+vwsxēḍśē₹ './gmnuh,y8+i9ul;kO):Y7jhf4efghyum/L_P/